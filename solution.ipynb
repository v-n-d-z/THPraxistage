{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Energy Benchmarking of Deep Learning Workflows**\n",
    "\n",
    "### **1.) Introduction to Energy Benchmarking with CodeCarbon**\n",
    "\n",
    "Researchers in the domains of Green AI or AI Efficiency have recenty started to include \"Energy Fact Sheets\" of the proposed models in their papers, similar to how the nutritional fact sheet on food items lists what is in the food you eat.\n",
    "\n",
    "This makes it possible for others to compare and grasp what kind of resources the researchers had at their disposal (GPUs, TPUs, ...), how many parameters their models have (Amount and FLOPs), how much energy was needed for a single training iteration or a single inference call, and sometimes also how many re-training attempts they did in total during their development.\n",
    "\n",
    "If direct hardware-based measuring of the energy usage is not possible or to much of an effort, software tools such as ***CodeCarbon*** are a good alternative, although generally less accurate. \n",
    "\n",
    "***CodeCarbon*** allows you to track the energy usage of your code with an intuitive *wrapping mechanism*. You can either use the explicit `EmissionsTracker` tracker object and wrap the code you want to benchmark with the `tracker.start()` and `tracker.stop()` functions. Or, if you already have your code bundled into individual functions, you can make use of the built-in function decorator `@track_emissions`.\n",
    "\n",
    "Both versions do the same things: \n",
    "1. Keep track of the starting time when you call `tracker.start()` or the decorated function\n",
    "2. Get an initial measurement of the energy usage at the start\n",
    "3. Start a scheduler in the background that does a measurement every X seconds while your code runs\n",
    "4. Wait until you either stop the tracking with `tracker.stop()` or the decorated function terminates\n",
    "5. Do another meaurement at the end\n",
    "6. Collect and aggregate the measurment data for you *(and store or return it to you depending on how you configured it)*\n",
    "\n",
    "\n",
    "#### **The following two ways are examples of how you can use this in your code:**\n",
    "##### **Version 1 - Tracker object**\n",
    "```python\n",
    "    from codecarbon import EmissionsTracker\n",
    "    \n",
    "    tracker = EmissionsTracker(\n",
    "        #... configurations\n",
    "    )\n",
    "\n",
    "    tracker.start()\n",
    "    try:\n",
    "        # ... do something   \n",
    "    finally:\n",
    "        tracker.stop()\n",
    "    \n",
    "    results = tracker.final_emissions_data\n",
    "```\n",
    "\n",
    "\n",
    "##### **Version 2 - Function decorator**\n",
    "\n",
    "```python\n",
    "    from codecarbon import track_emissions\n",
    "\n",
    "    @track_emissions(\n",
    "        #... configurations\n",
    "    )\n",
    "    def func()\n",
    "        # ... do something\n",
    "```\n",
    "\n",
    "To access the energy benchmark data, you can either look into the `.csv` file that is created by default. Every row corresponds to one call of `start()` to `stop()`. Alternatively, and more useful if you want to directly do something with the respective value, you can access the `final_emissions_data` attribute of the tracker object (This of course only works if you explicitly use the tracker object and not the function decorator).\n",
    "\n",
    "### **2.) Energy Benchmarking of Model Training and Testing**\n",
    "\n",
    "Understanding the energy usage of AI models is important for the reduction of the immense carbon footprint of modern AI and making AI development, in general, more sustainable. \n",
    "\n",
    "For this next part of the notebook, we will train deep learning models on the Oxford IIIT Pets dataset (see [here](https://pytorch.org/vision/stable/generated/torchvision.datasets.OxfordIIITPet.html) and [here](https://www.robots.ox.ac.uk/~vgg/data/pets/)). Your task is to train a deep learning model, achieve an accuracy of at least 80% and find ways to make the training phase of the model as energy efficient as possible.\n",
    "\n",
    "We will start by implementing the two helper functions `train_model()` and `test_model()`. These should make it easier and faster for you to go over multiple iterations of your models and try different approaches. Once the basic functionality is implemented, you can extend them how ever you please.\n",
    "\n",
    "#### **Getting Started**\n",
    "\n",
    "1. Set up your virtual environment for python and run the import cell down below\n",
    "2. Implement the leftout spaces of the train_model function\n",
    "3. Implement a basic first model to test and familiarize yourself with how the two functions work (also helps to get a baseline for the accuracy and energy usage)\n",
    "4. Extend your models and the two functions to increase the energy efficiency \n",
    "    - have a look at ***Things you can consider*** down below to get some ideas where to start.\n",
    "    - have a look at the basic implementation of the `test_model()` function to get an understanding of what we expect from you in the `train_model()` function that you have to implement yourself.\n",
    "\n",
    "**Things you can consider:**\n",
    "\n",
    "- Run model on CPU vs. GPU (see [here](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html) and [here](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device))\n",
    "- Model architecture and size ([custom models](https://pytorch.org/tutorials/beginner/introyt/modelsyt_tutorial.html) vs. [built-in Pytorch models](https://pytorch.org/vision/stable/models.html))\n",
    "- Pretrained model weights vs. starting from scratch\n",
    "- Different optimizers (see [here](https://pytorch.org/docs/stable/optim.html))\n",
    "- Learning rate and adaptation strategies (Warm-start, Decay, ...)\n",
    "- Batch size\n",
    "- Data augmentation (see [here](https://pytorch.org/vision/stable/transforms.html))\n",
    "- Different loss functions\n",
    "- Train/Test vs. Train/Validation/Test split of the data set. What could a additional validation set be useful for? (*Hint: Early Stopping*)\n",
    "\n",
    "Due to the limited time you have, do not worry about implementing or trying out all of these examples. Start by discussing with your colleagues which of these examples help the model to be more energy efficient, and if so, how they achieve it.\n",
    "\n",
    "Afterwards, pick and focus on the few that you think would help the most (or that you simply find the most interesting to implement). You are, of course, also allowed to come up with your own strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from codecarbon import EmissionsTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `train_model()` function\n",
    "This function should take in a PyTorch model, a string name for your model and a PyTorch DataLoader instance for the Oxford Pets dataset. In the end, it should take care of the complete model training process for you so that you can focus your energy on how and what to change in your models. (*Hint: as you might have seen in the **Things to consider** section, there are also a few things you can improve over basic the train_model() function to make the training phase a lot more efficient*)\n",
    "\n",
    "In short, we want you to implement the standard deep learning training loop and afterwards expand it how ever you please:\n",
    "- loop over the epochs and batches of input images and labels\n",
    "- calculate the loss of your model predictions\n",
    "- backpropagte the loss\n",
    "- do the optimizer weight update step\n",
    "\n",
    "Additionally, this is where you should utilize the ***CodeCarbon*** benchmarking library, to understand how much time and energy the training phase of your models need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, model_name:str, train_loader:DataLoader, val_loader:DataLoader, num_epochs=20, threshold=0.9, lr=1e-4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Reduced learning rate when validation accuracy starts to plateau\n",
    "    lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.7)\n",
    "\n",
    "    tracker = EmissionsTracker(\n",
    "        project_name=\"Model_Training\",\n",
    "        output_file=\"train_emissions.csv\",\n",
    "        measure_power_secs=20,\n",
    "        log_level=\"critical\",\n",
    "        allow_multiple_runs=True\n",
    "    )\n",
    "    tracker.start()\n",
    "    start_time = time.perf_counter()\n",
    "    try:\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        best_acc = 0.0\n",
    "        for epoch in range(num_epochs):\n",
    "            running_loss = 0.0\n",
    "            correct = total = 0\n",
    "            for images, labels in train_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad() # Manually zero the accumulated gradients\n",
    "                outputs = model(images)\n",
    "                loss = loss_func(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * images.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "            \n",
    "            epoch_loss = running_loss / len(train_loader.dataset)\n",
    "            epoch_acc = correct / total\n",
    "            \n",
    "            # Reduce learning rate when validation accuracy starts to plateau\n",
    "            lr_scheduler.step()\n",
    "            \n",
    "            # Validation phase every 5th epoch\n",
    "            if epoch % 5 == 4:\n",
    "                model.eval()  # Set model to evaluation mode\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                \n",
    "                with torch.no_grad():  # No gradients needed for validation\n",
    "                    for images, labels in val_loader:\n",
    "                        images, labels = images.to(device), labels.to(device)\n",
    "                        \n",
    "                        outputs = model(images)\n",
    "                        _, predicted = torch.max(outputs, 1)\n",
    "                        val_correct += (predicted == labels).sum().item()\n",
    "                        val_total += labels.size(0)\n",
    "                        images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "                \n",
    "                val_acc = val_correct / val_total\n",
    "                print(f\"  Epoch {epoch+1:3d}/{num_epochs} - Train Loss: {epoch_loss:.5f} | Train Accuracy: {100*epoch_acc:.1f}% | Validation Accuracy: {100*val_acc:.1f}%\")\n",
    "                \n",
    "                # Save the best model based on validation accuracy\n",
    "                if val_acc > best_acc:\n",
    "                    best_acc = val_acc\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                # If the validation accuracy reaches the threshold, stop training\n",
    "                if val_acc >= threshold:\n",
    "                    print(f\"  Reached target validation accuracy of {threshold * 100}% at epoch {epoch+1}. Model training stopped.\")\n",
    "                    break\n",
    "\n",
    "        print(f\"  Trained for {epoch+1} epochs - Train Loss: {epoch_loss:.5f} | Train Accuracy: {100*epoch_acc:.1f}% | Validation Accuracy: {100*val_acc:.1f}%\")\n",
    "\n",
    "        training_time = time.perf_counter() - start_time\n",
    "        \n",
    "        tracker.stop()\n",
    "        emissions_data = tracker.final_emissions_data\n",
    "\n",
    "        print(f\"[Training] Time: {training_time:.2f} seconds | CO2 Emissions: {emissions_data.emissions:.6f} kg | Total Energy Usage: {emissions_data.energy_consumed:.7f} kWh\")\n",
    "        \n",
    "        # Load the best model weights\n",
    "        model.load_state_dict(best_model_wts)\n",
    "\n",
    "        # Store the model\n",
    "        models_path = \"./models/\"\n",
    "        os.makedirs(models_path, exist_ok=True)\n",
    "        torch.save(model.state_dict(), models_path + model_name + f\"_val-acc-{val_acc}\")\n",
    "\n",
    "        return model\n",
    "    \n",
    "    finally:\n",
    "        tracker.stop()\n",
    "        model.to(\"cpu\")\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `test_model()` function\n",
    "This function again takes in a PyTorch model and a PyTorch DataLoader instance of the test split of the dataset, calculates the test accuracy and the inference latency and energy usage at test time of your models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_dataloader):\n",
    "    # Send model to gpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Set model to inference mode\n",
    "    model.eval()\n",
    "\n",
    "    tracker = EmissionsTracker(\n",
    "        project_name=\"Model_Inference\",\n",
    "        output_file=\"inference_emissions.csv\",\n",
    "        measure_power_secs=20,\n",
    "        log_level=\"error\",\n",
    "        allow_multiple_runs=True\n",
    "    )\n",
    "    tracker.start()\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "    try:\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_dataloader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                images, labels = images.to(\"cpu\"), labels.to(\"cpu\")\n",
    "        \n",
    "        total_inference_time = time.perf_counter() - start_time\n",
    "\n",
    "        avg_inference_latency = total_inference_time / len(test_dataloader.dataset)\n",
    "        test_acc = 100 * correct / total\n",
    "        \n",
    "        tracker.stop()\n",
    "        emissions_data = tracker.final_emissions_data\n",
    "\n",
    "        print(f\"[Inference] #Test Samples: {len(test_dataloader.dataset)} | Average Inference Latency: {avg_inference_latency:.4f} seconds | Test Accuracy: {test_acc:.1f}%\") \n",
    "        print(f\"  - CO2 Emissions: {emissions_data.emissions:.6f} kg | Total Energy Usage: {emissions_data.energy_consumed:.7f} kWh\")\n",
    "        return {\n",
    "            \"inference_latency\": avg_inference_latency,\n",
    "            \"test_accuracy\": test_acc,\n",
    "            \"emissions_data\": emissions_data\n",
    "        }\n",
    "\n",
    "    finally:\n",
    "        tracker.stop()\n",
    "        model.to(\"cpu\")\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally create, train and test your models here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "output_size = 37 # Oxford Pets 37 categories\n",
    "\n",
    "# Data set transforms for the Oxford Pets Dataset\n",
    "transforms = v2.Compose([\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    v2.Resize((224,224)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "train_dataset_complete = datasets.OxfordIIITPet(root=\"./data\", split=\"trainval\", transform=transforms, download=True)\n",
    "test_dataset = datasets.OxfordIIITPet(root=\"./data\", split=\"test\", transform=transforms, download=True)\n",
    "\n",
    "# Split train dataset into train and validation\n",
    "train_dataset, validation_dataset = random_split(train_dataset_complete, [0.9, 0.1])\n",
    "\n",
    "# Setup data loaders to batch image and labels\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_dataloader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Training ResNet34 with pretrained weights\n",
      "  Epoch   5/20 - Train Loss: 0.05675 | Train Accuracy: 99.6% | Validation Accuracy: 95.7%\n",
      "  Epoch  10/20 - Train Loss: 0.00059 | Train Accuracy: 100.0% | Validation Accuracy: 94.6%\n",
      "  Epoch  15/20 - Train Loss: 0.00034 | Train Accuracy: 100.0% | Validation Accuracy: 95.1%\n",
      "  Epoch  20/20 - Train Loss: 0.00033 | Train Accuracy: 100.0% | Validation Accuracy: 95.4%\n",
      "  Trained for 20 epochs - Train Loss: 0.00033 | Train Accuracy: 100.0% | Validation Accuracy: 95.4%\n",
      "[Training] Time: 418.10 seconds | CO2 Emissions: 0.007911 kg | Total Energy Usage: 0.0207654 kWh\n"
     ]
    }
   ],
   "source": [
    "### Create your model\n",
    "    # You can either build one from scratch using a custom layer setup, or try builtin model architectures from PyTorch (ResNet, MobileNet, ...)\n",
    "    # Best start with a simple model to test the training and testing workflow and then work up to more complex models\n",
    "print(\"### Training ResNet34 with pretrained weights\")\n",
    "resnet34_pre = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
    "resnet34_pre.fc = nn.Linear(resnet34_pre.fc.in_features, output_size)\n",
    "\n",
    "resnet34_pre = train_model(resnet34_pre, \"resnet_34_pre\", train_dataloader, validation_dataloader, lr=1e-4, num_epochs=20, threshold=0.99)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Inference] #Test Samples: 3669 | Average Inference Latency: 0.0054 seconds | Test Accuracy: 89.2%\n",
      "  - CO2 Emissions: 0.000259 kg | Total Energy Usage: 0.0006798 kWh\n"
     ]
    }
   ],
   "source": [
    "results = test_model(resnet34_pre, test_dataloader)\n",
    "\n",
    "resnet34_pre.to(\"cpu\")\n",
    "del resnet34_pre\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
